# Step-by-step implementation to build a trauma-sensitive chatbot using OpenAI, Streamlit, and a Hugging Face dataset

# ------------------------------------------
# 1. Install dependencies (use in terminal)
# pip install openai streamlit llama-index datasets
# ------------------------------------------

import os
import openai
import streamlit as st
from datasets import load_dataset
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document
from llama_index.core import StorageContext, load_index_from_storage

PERSIST_DIR = "./storage/VectorStoreIndex/"

# Load Hugging Face Counsel Chat dataset
@st.cache_resource(show_spinner=True)
def load_and_prepare_documents():
    dataset = load_dataset("nbertagnolli/counsel-chat")
    documents = []


# Load Hugging Face Counsel Chat dataset
@st.cache_resource(show_spinner=True)
def load_and_prepare_documents():
    dataset = load_dataset("nbertagnolli/counsel-chat")
    documents = []
    for idx, item in enumerate(dataset["train"]):
        question = item.get("questionText", "").strip()
        answer = item.get("answerText", "").strip()
        if question and answer:
            text = f"Patient: {question}\nTherapist: {answer}"
            documents.append(Document(text=text, doc_id=str(idx)))
    return documents


# Build and persist index if not yet stored
if not os.path.exists(PERSIST_DIR):
    os.makedirs(PERSIST_DIR, exist_ok=True)
    docs = load_and_prepare_documents()
    index = VectorStoreIndex.from_documents(docs)
    index.storage_context.persist(persist_dir=PERSIST_DIR)

# Streamlit Interface
st.title("üí¨ Trauma-Sensitive Chatbot")
st.caption("Supporting emotional reflection with curated counselor responses")

# Sidebar for OpenAI API Key
with st.sidebar:
    openai_api_key = st.text_input("üîê OpenAI API Key", key="api_key", type="password")
    if openai_api_key:
        os.environ["API KEY"] = openai_api_key
        st.success("API key set!")
    else:
        st.warning("Please add your OpenAI API key to continue.")
        st.stop()

# Load Index
try:
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    vectorstoreindex = load_index_from_storage(storage_context)
    chat_engine = vectorstoreindex.as_chat_engine(
        chat_mode="context",
        system_prompt=(
            "You are a trauma-sensitive counseling assistant.\n"
            "Use calm, empathetic, and supportive language.\n"
            "Refer to retrieved counselor responses as guidance, not templates."
        ),
        verbose=True
    )
except Exception as e:
    st.error(f"Could not load index: {e}")
    st.stop()

# Chat message history
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello. I'm here to support your emotional journey. How can I help you today?"}
    ]

# User Input
if prompt := st.chat_input("Ask your question..."):
    st.session_state.messages.append({"role": "user", "content": prompt})

# Display messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# Generate assistant reply
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking empathetically..."):
            try:
                reply = chat_engine.chat(st.session_state.messages[-1]["content"])
                st.write(reply.response)
                st.session_state.messages.append({"role": "assistant", "content": reply.response})
            except Exception as e:
                st.error(f"Chat generation failed: {e}")
